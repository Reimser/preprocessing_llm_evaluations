import os
import pandas as pd
from transformers import GPT2TokenizerFast

# GPT2-Tokenizer zum TokenzÃ¤hlen
tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")

# Parameter fÃ¼r Eval-Subset
MIN_TOKENS = 400
MAX_TOKENS = 800
SAMPLE_SIZE = 3
RAW_PATH = "../data/raw/cnn.csv"
SAVE_PATH = "../data/eval/eval_subset.csv"

def estimate_tokens(text):
    return len(tokenizer.encode(text))

def create_eval_subset():
    print("ðŸ“„ Lade lokale Datei:", RAW_PATH)
    df = pd.read_csv(RAW_PATH)

    print("ðŸ”Ž Filtere nach TokenlÃ¤nge...")
    df["token_count"] = df["article"].apply(estimate_tokens)
    filtered_df = df[
        (df["token_count"] >= MIN_TOKENS) &
        (df["token_count"] <= MAX_TOKENS)
    ]

    if len(filtered_df) < SAMPLE_SIZE:
        print(f"âš ï¸ Nur {len(filtered_df)} passende Artikel gefunden. SAMPLE_SIZE wird angepasst.")
        sample_size = len(filtered_df)
    else:
        sample_size = SAMPLE_SIZE

    sampled_df = filtered_df.sample(n=sample_size, random_state=42).drop(columns=["token_count"])
    os.makedirs(os.path.dirname(SAVE_PATH), exist_ok=True)
    sampled_df.to_csv(SAVE_PATH, index=False)

    print(f"âœ… Eval-Subset ({sample_size} Artikel) gespeichert unter: {SAVE_PATH}")

if __name__ == "__main__":
    create_eval_subset()
